# dnn_activation
We investigate the relationship between [DNN (Deep Neural Network) approximation](https://doi.org/10.1016/0893-6080(89)90020-8) and activation function selection. This repo is a supplementary material for [author's blog post (written Japanese)](link). 

## Purpose
Activation functions introduce non-linearity to DNN approximation i.e. DNN approximations are heavily dependent on the properties of the selected activation functions. This repo's codes learns several functions with 5 different activation functions, namely, ReLU, ELU, Swish, sin, and tanh. 

## Reference
[1] Kurt Hornik, Maxwell Stinchcombe, Halbert White: Multilayer feedforward networks are universal approximators, *Neural Networks*, Vol. 2, No. 5, pp. Pages 359-366, 1989. ([paper](https://doi.org/10.1016/0893-6080(89)90020-8))
<br>
[2] [author's blog post (written Japanese)](link). 
